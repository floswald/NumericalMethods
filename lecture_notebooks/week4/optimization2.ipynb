{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Optimization 2: Algorithms and Constraints\n",
    "\n",
    "Florian Oswald\n",
    "Sciences Po, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bracketing\n",
    "\n",
    "* A derivative-free method for *univariate* $f$\n",
    "* works only on **unimodal** function $f$\n",
    "* We can find a *bracket* in which the global minimum exists if we can find points $a < b < c$ such that\n",
    "\n",
    "$$f(a) \\geq f(b) < f(c) \\text{ or } f(a) > f(b) \\leq f(c)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](unimodal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Initial bracket\n",
    "\n",
    "* To start with, we need to identify first a region where bracketing will work.\n",
    "* Once we found a valid interval, we shrink it until we find the optimimum.\n",
    "* Here is an algorithm which will find a valid bracket, taking successive steps into a direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* The function `bracket_minimum` (next slide) in action:\n",
    "\n",
    "![](bracketing.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# algo 3.1 from Kochenderfer and Wheeler\n",
    "function bracket_minimum(f, x=0; s=1e-2, k=2.0) \n",
    "    a, ya = x, f(x)\n",
    "    b, yb = a + s, f(a + s) \n",
    "    if yb > ya\n",
    "        a, b = b, a\n",
    "        ya, yb = yb, ya s = -s\n",
    "    end\n",
    "    while true\n",
    "    c, yc = b + s, f(b + s) \n",
    "        if yc > yb\n",
    "            return a < c ? (a, c) : (c, a) \n",
    "        end\n",
    "        a, ya, b, yb = b, yb, c, yc\n",
    "        s *= k \n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Fibonacci Search\n",
    "\n",
    "* Suppose we have a bracket $[a,b]$ on $f$ and that we can only evaluate $f$ twice.\n",
    "* Where to evaluate it?\n",
    "* Let's split the interval into 3 equal pieces:\n",
    "\n",
    "![](fibo1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* If we move the points to the center, we can do better:\n",
    "\n",
    "![](fibo2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* for $n$ queries of $f$, we get the following sequence\n",
    "* This can be determined analytically as $F_n = \\frac{\\psi^n - (1-\\psi)^n}{\\sqrt{5}}$\n",
    "* and $\\psi = (1+\\sqrt{5})/2$ is called the *golden ratio*, which determines successive steps in the *Fibonacci Sequence*:\n",
    "\n",
    "$$\\frac{F_n}{F_{n-1}} = \\psi \\frac{1-s^{n+1}}{1-s^n}, \\quad s=\\frac{1-\\sqrt{5}}{1+\\sqrt{5}}\\approx-0.382$$\n",
    "\n",
    "![](fibo3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* *Golden Section Search* uses the *golden ratio* to approximate the Fibonacci sequence.\n",
    "* This trades off a minimal number of function evaluations with the best placement of evaluation points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](golden1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](golden2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "julia"
     ],
     "id": ""
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "using Plots\n",
    "f(x) = exp(x) - x^4\n",
    "plot(f,0,2, label = \"f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Optim.jl\n",
    "\n",
    "* Let's optimize this function with Optim!\n",
    "* A typical call looks like this:\n",
    "\n",
    "```julia\n",
    "function your_function(x) ... end\n",
    "using Optim\n",
    "optimize(your_function,from,to,which_method_to_use)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "using Optim\n",
    "minf(x) = -f(x)\n",
    "brent = optimize(minf,0,2,Brent())\n",
    "golden = optimize(minf,0,2,GoldenSection())\n",
    "vline!([Optim.minimizer(brent)],label = \"brent\")\n",
    "vline!([Optim.minimizer(golden)], label = \"golden\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Using Bracketing methods on non-unimodal functions is a *bad idea*\n",
    "* We identify a different optimum depending on where we start to search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "f2 = x->sin(x) + 0.1*abs(x)\n",
    "x_arr = collect(range(3π/2-8.5, stop=3π/2+5.5, length=151))\n",
    "y_arr = f2.(x_arr) \n",
    "plot(x_arr,y_arr,legend = false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "ob = optimize(f2,1,5,Brent())\n",
    "vline!([Optim.minimizer(ob)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "ob = optimize(f2,-2.5,2.5,Brent())\n",
    "vline!([Optim.minimizer(ob)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "ob = optimize(f2,-2.5,10,Brent())\n",
    "vline!([Optim.minimizer(ob)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Bisection Methods\n",
    "\n",
    "* Root finding: `Roots.jl`\n",
    "* Root finding in multivariate functions: [`IntervalRootFinding.jl`](https://github.com/JuliaIntervals/IntervalRootFinding.jl/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "using Roots\n",
    "# find the zeros of this function:\n",
    "f(x) = exp(x) - x^3\n",
    "plot(f,-2,5, label = \"f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fzero(f, -2,4)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fzero(f,4,5)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "using IntervalRootFinding, IntervalArithmetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "rts = roots(f, -2..5, IntervalRootFinding.Bisection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Rosenbrock Banana and Optim.jl\n",
    "\n",
    "* We can supply the objective function and - depending on the solution algorithm - the gradient and hessian as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "julia"
     ],
     "id": ""
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "using Optim\n",
    "using OptimTestProblems\n",
    "for (name, prob) in MultivariateProblems.UnconstrainedProblems.examples\n",
    "   println(name)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "prob = UnconstrainedProblems.examples[\"Rosenbrock\"]\n",
    "ro = prob.f\n",
    "g! = prob.g!\n",
    "h! = prob.h!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Comparison Methods\n",
    "\n",
    "* We will now look at a first class of algorithms, which are very simple, but sometimes a good starting point.\n",
    "* They just *compare* function values.\n",
    "* *Grid Search* : Compute the objective function at $G=\\{x_1,\\dots,x_N\\}$ and pick the highest value of $f$. \n",
    "\t* This is very slow.\n",
    "\t* It requires large $N$.\n",
    "\t* But it's robust (will find global optimizer for large enough $N$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "julia"
     ],
     "id": ""
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# grid search on rosenbrock\n",
    "grid = collect(-1.0:0.1:3);  # grid spacing is important!\n",
    "grid2D = [[i;j] for i in grid,j in grid];\n",
    "val2D = map(ro,grid2D);\n",
    "r = findmin(val2D);\n",
    "println(\"grid search results in minimizer = $(grid2D[r[2]])\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Local Descent Methods\n",
    "\n",
    "* Applicable to multivariate problems\n",
    "* We are searching for a *local model* that provides some guidance in a certain region of $f$ over **where to go to next**.\n",
    "* Gradient and Hessian are informative about this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Local Descent Outline\n",
    "\n",
    "All descent methods follow more or less this structure. At iteration $k$,\n",
    "\n",
    "1. Check if candidate $\\mathbf{x}^{(k)}$ satisfies stopping criterion:\n",
    "    * if yes: stop\n",
    "    * if no: continue\n",
    "2. Get the local *descent direction*  $\\mathbf{d}^{(k)}$, using gradient, hessian, or both.\n",
    "3. Set the *step size*, i.e. the length of the next step, $\\alpha^k$\n",
    "4. Get the next candidate via\n",
    "    $$\\mathbf{x}^{(k+1)} \\longleftarrow \\alpha^k\\mathbf{d}^{(k)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Line Search Strategy\n",
    "\n",
    "* An algorithm from the line search class  chooses a direction $\\mathbf{d}^{(k)} \\in \\mathbb{R}^n$ and searches along that direction starting from the current iterate $x_k \\in \\mathbb{R}^n$ for a new iterate $x_{k+1} \\in \\mathbb{R}^n$ with a lower function value.\n",
    "* After deciding on a direction $\\mathbf{d}^{(k)}$, one needs to decide the *step length* $\\alpha$ to travel by solving\n",
    "\t$$ \\min_{\\alpha>0} f(x_k + \\alpha \\mathbf{d}^{(k)}) $$\n",
    "* In practice, solving this exactly is too costly, so algos usually generate a sequence of trial values $\\alpha$ and pick the one with the lowest $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](line-search.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "using LineSearches\n",
    "\n",
    "algo_hz = Optim.Newton(linesearch = HagerZhang())    # Both Optim.jl and IntervalRootFinding.jl export `Newton`\n",
    "res_hz = Optim.optimize(ro, g!, h!, prob.initial_x, method=algo_hz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Trust Region Strategy\n",
    "\n",
    "* First choose max step size, then the direction\n",
    "* Finds the next step $\\mathbf{x}^{(k+1)}$ by minimizing a model of $\\hat{f}$ over a *trust region*, centered on $\\mathbf{x}^{(k)}$\n",
    "    * 2nd order Tayloer approx of $f$ is common.\n",
    "* Radius $\\delta$ of trust region is changed based on how well $\\hat{f}$ fits $f$ in trust region.\n",
    "* Get $\\mathbf{x'}$ via\n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "    \\min_{\\mathbf{x'}} &\\quad \\hat{f}(\\mathbf{x'}) \\\\\n",
    "    \\text{subject to } &\\quad \\Vert \\mathbf{x}-\\mathbf{x'} \\leq \\delta \\Vert\n",
    "    \\end{aligned}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](trust-region.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Optim.jl has a TrustRegion for Newton (see below for Newton's Method)\n",
    "NewtonTrustRegion(; initial_delta = 1.0, # The starting trust region radius\n",
    "                    delta_hat = 100.0, # The largest allowable trust region radius\n",
    "                    eta = 0.1, #When rho is at least eta, accept the step.\n",
    "                    rho_lower = 0.25, # When rho is less than rho_lower, shrink the trust region.\n",
    "                    rho_upper = 0.75) # When rho is greater than rho_upper, grow the trust region (though no greater than delta_hat).\n",
    "res = Optim.optimize(ro, g!, h!, prob.initial_x, method=NewtonTrustRegion())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](trust-region2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Stopping criteria\n",
    "\n",
    "1. maximum number of iterations reached\n",
    "2. absolute improvement $|f(x) - f(x')| \\leq \\epsilon$\n",
    "3. relative improvement $|f(x) - f(x')| / |f(x)| \\leq \\epsilon$\n",
    "4. Gradient close to zero $|g(x)| \\approx 0$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient Descent\n",
    "\n",
    "The steepest descent is *opposite the gradient*.\n",
    "* Here we define\n",
    "    $$\\mathbf{g}^{(k)} = \\nabla f(\\mathbf{d}^{(k)})$$\n",
    "* And our descent becomes\n",
    "    $$\\mathbf{d}^{(k)} = -\\nabla \\frac{\\mathbf{g}^{(k)} }{\\Vert\\mathbf{g}^{(k)}\\Vert }$$\n",
    "* Minimizing wrt step size results in a jagged path (each direction is orthogonal to previous direction!) See Kochenderfer and Wheeler chapter 5, equation (5.6)\n",
    "    $$\\alpha^{(k)} = \\arg \\min{\\alpha} f(\\mathbf{x}^{(k)} + \\alpha \\mathbf{d}^{(k)}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Optim.jl again: Default constructor for a GradientDescent solver\n",
    "GradientDescent(; alphaguess = LineSearches.InitialPrevious(),\n",
    "                  linesearch = LineSearches.HagerZhang(),\n",
    "                  P = nothing,\n",
    "                  precondprep = (P, x) -> nothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Those cool graphs are modified from Patrick Kofod Mogensens' Julia Con presentation\n",
    "# https://github.com/pkofod/JC2017\n",
    "# some are identical copies.\n",
    "x = [-4,3.]\n",
    "res = optimize(ro, x, GradientDescent(), Optim.Options(store_trace=true, extended_trace=true))\n",
    "@manipulate for ix = slider(1:Optim.iterations(res), value = 1)\n",
    "    contour(-4.1:0.01:2, -1.5:0.01:4.1, (x,y)->sqrt(ro([x, y])), fill=true, color=:deep, legend=false)\n",
    "    xtracemat = hcat(Optim.x_trace(res)...)\n",
    "    plot!(xtracemat[1, 1:ix], xtracemat[2, 1:ix], mc = :white, lab=\"\")\n",
    "    scatter!(xtracemat[1:1,ix], xtracemat[2:2,ix], mc=:black, msc=:red, lab=\"\")\n",
    "    scatter!([1.], [1.], mc=:red, msc=:red, lab=\"\")\n",
    "    scatter!(x[1:1], x[2:2], mc=:yellow, msc=:black, label=\"start\", legend=true)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# zooming in: along the valley, it moves really slow!\n",
    "x = [-1,1.]\n",
    "res = optimize(ro, x, GradientDescent(), Optim.Options(store_trace=true, extended_trace=true))\n",
    "@manipulate for ix = slider(1:Optim.iterations(res), value = 1)\n",
    "    contour(-2.5:0.01:2, -1.5:0.01:2, (x,y)->sqrt(ro([x, y])), fill=true, color=:deep, legend=false)\n",
    "    xtracemat = hcat(Optim.x_trace(res)...)\n",
    "    plot!(xtracemat[1, 1:ix], xtracemat[2, 1:ix], mc = :white, lab=\"\")\n",
    "    scatter!(xtracemat[1:1,ix], xtracemat[2:2,ix], mc=:black, msc=:red, lab=\"\")\n",
    "    scatter!([1.], [1.], mc=:red, msc=:red, lab=\"\")\n",
    "    scatter!(x[1:1], x[2:2], mc=:yellow, msc=:black, label=\"start\", legend=true)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* pretty zig-zaggy\n",
    "* it may not be a huge issue for final convergence, but we take a lot of *unncessary* steps back and forth.\n",
    "* so in terms of computational cost, we incur quite a lot here.\n",
    "* zoom in even more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "x = [-4, -3.]\n",
    "@manipulate for ix = slider(120:160, value = 120)\n",
    "    c = contour(-.85:0.01:-0.60, 0:0.01:1.2, (x,y)->sqrt(ro([x, y])), fill=true, color=:deep, legend=false)\n",
    "    res = optimize(ro, x, GradientDescent(), Optim.Options(store_trace=true, extended_trace=true))\n",
    "    xtracemat = hcat(Optim.x_trace(res)...)\n",
    "    \n",
    "    plot!(xtracemat[1, 120:ix], xtracemat[2, 120:ix], mc = :white, legend=true, label=\"Gradient Descent\")\n",
    "    scatter!(xtracemat[1:1,ix], xtracemat[2:2,ix], mc=:black, msc=:red, label = \"\")\n",
    "    scatter!(xtracemat[1:1,120], xtracemat[2:2,120], mc=:blue, msc=:blue, label=\"start\")\n",
    "    p = plot(1:ix, [Optim.trace(res)[i].metadata[\"Current step size\"] for i  = 1:ix], label = \"step size\")\n",
    "    plot(c, p, layout=(2,1))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Conjugate Gradient Descent\n",
    "\n",
    "* Tries to avoid the jagged path problem\n",
    "* It combines several methods: it minimizes a quadratic form of $f$\n",
    "* The next descent direction uses the gradient *plus* some additional info from the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "x = [-4,3.]\n",
    "res = optimize(ro, x, ConjugateGradient(), Optim.Options(store_trace=true, extended_trace=true))\n",
    "@manipulate for ix = slider(1:Optim.iterations(res), value = 1)\n",
    "    contour(-4.1:0.01:2, -1.5:0.01:4.1, (x,y)->sqrt(ro([x, y])), fill=true, color=:deep, legend=false)\n",
    "    xtracemat = hcat(Optim.x_trace(res)...)\n",
    "    plot!(xtracemat[1, 1:ix], xtracemat[2, 1:ix], mc = :white, lab=\"\")\n",
    "    scatter!(xtracemat[1:1,ix], xtracemat[2:2,ix], mc=:black, msc=:red, lab=\"\")\n",
    "    scatter!([1.], [1.], mc=:red, msc=:red, lab=\"\")\n",
    "    scatter!(x[1:1], x[2:2], mc=:yellow, msc=:black, label=\"start\", legend=true)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = [-4, -3.]\n",
    "i0 = 15\n",
    "res = optimize(ro, x0, ConjugateGradient(), Optim.Options(store_trace=true, extended_trace=true))\n",
    "\n",
    "@manipulate for ix = slider(i0:Optim.iterations(res), value = i0)\n",
    "    c = contour(0.5:0.01:1.1, 0:0.01:1.1, (x,y)->sqrt(ro([x, y])), fill=true, color=:deep, legend=false)\n",
    "    xtracemat = hcat(Optim.x_trace(res)...)\n",
    "    \n",
    "    plot!(xtracemat[1, i0:ix], xtracemat[2, i0:ix], mc = :white, legend=:bottomright, label=\"Gradient Descent\")\n",
    "    scatter!(xtracemat[1:1,ix], xtracemat[2:2,ix], mc=:black, msc=:red, label = \"\")\n",
    "    scatter!(xtracemat[1:1,i0], xtracemat[2:2,i0], mc=:blue, msc=:blue, label=\"start\")\n",
    "    scatter!([1.], [1.], mc=:red, msc=:red, lab=\"\")\n",
    "\n",
    "    p = plot(1:ix, [Optim.trace(res)[i].metadata[\"Current step size\"] for i  = 1:ix], label = \"step size\")\n",
    "    plot(c, p, layout=(2,1))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Second Order Methods\n",
    "\n",
    "### Newton's Method\n",
    "\n",
    "* We start with a 2nd order Taylor approx over x at step $k$:\n",
    "    $$q(x) = f(x^{(k)}) + (x - x^{(k)}) f'(x^{(k)}) + \\frac{(x - x^{(k)})^2}{2}f''(x^{(k)})$$\n",
    "* We form first order conditions (set it's root equal to zero) and rearrange to find the next step $k+1$:\n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "    \\frac{\\partial q(x)}{\\partial x} &= f'(x^{(k)}) + (x - x^{(k)}) f''(x^{(k)}) = 0 \\\\\n",
    "    x^{(k+1)} &= x^{(k)} - \\frac{f'(x^{(k)})}{f''(x^{(k)})}\n",
    "    \\end{aligned}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* The same argument works for multidimensional functions by using Hessian and Gradient\n",
    "* We would get a descent $\\mathbf{d}^k$ by setting:\n",
    "    $$\\mathbf{d}^k = -\\frac{\\mathbf{g}^{k}}{\\mathbf{H}^{k}}$$\n",
    "* There are several options to avoid (often costly) computation of the Hessian $\\mathbf{H}$:\n",
    "1. Quasi-Newton updates $\\mathbf{H}$ starting from identity matrix\n",
    "2. Broyden-Fletcher-Goldfarb-Shanno (BFGS) does better with approx linesearch\n",
    "3. L-BFGS is the limited memory version for large problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "x = [-1., -1.]\n",
    "res = optimize(ro, x, GradientDescent(), Optim.Options(store_trace=true, extended_trace=true))\n",
    "res2 = optimize(ro, g!, h!, x, Optim.Newton(), Optim.Options(store_trace=true, extended_trace=true))\n",
    "@manipulate for ix = slider(1:Optim.iterations(res2), value = 1)\n",
    "    p = contour(-2.5:0.01:2, -1.5:0.01:2, (x,y)->sqrt(ro([x, y])), fill=true, color=:deep, legend=false)\n",
    "    xtracemat = hcat(Optim.x_trace(res)...)\n",
    "    plot!(xtracemat[1, 1:ix], xtracemat[2, 1:ix], mc = :white,  label=\"Gradient Descent\", legend=true)\n",
    "    scatter!(xtracemat[1:1,ix], xtracemat[2:2,ix], mc=:black, msc=:red, label=\"\")\n",
    "    \n",
    "    xtracemat2 = hcat(Optim.x_trace(res2)...)\n",
    "    plot!(xtracemat2[1, 1:ix], xtracemat2[2, 1:ix], c=:blue, label=\"Newton\")\n",
    "    scatter!(xtracemat2[1:1,ix], xtracemat2[2:2,ix], mc=:black, msc=:blue, label=\"\")\n",
    "    scatter!([1.], [1.], mc=:red, msc=:red, label=\"\")\n",
    "    scatter!(x[1:1], x[2:2], mc=:yellow, msc=:yellow, label=\"start\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [-1., -1.]\n",
    "res = optimize(ro, x, ConjugateGradient(), Optim.Options(store_trace=true, extended_trace=true))\n",
    "res2 = optimize(ro, g!, h!, x, Optim.Newton(), Optim.Options(store_trace=true, extended_trace=true))\n",
    "@manipulate for ix = slider(1:Optim.iterations(res2), value = 1)\n",
    "    p = contour(-2.5:0.01:2, -1.5:0.01:2, (x,y)->sqrt(ro([x, y])), fill=true, color=:deep, legend=false)\n",
    "    xtracemat = hcat(Optim.x_trace(res)...)\n",
    "    plot!(xtracemat[1, 1:ix], xtracemat[2, 1:ix], mc = :white,  label=\"Conjugate Gradient\", legend=true)\n",
    "    scatter!(xtracemat[1:1,ix], xtracemat[2:2,ix], mc=:black, msc=:red, label=\"\")\n",
    "    \n",
    "    xtracemat2 = hcat(Optim.x_trace(res2)...)\n",
    "    plot!(xtracemat2[1, 1:ix], xtracemat2[2, 1:ix], c=:blue, label=\"Newton\")\n",
    "    scatter!(xtracemat2[1:1,ix], xtracemat2[2:2,ix], mc=:black, msc=:blue, label=\"\")\n",
    "    scatter!([1.], [1.], mc=:red, msc=:red, label=\"\")\n",
    "    scatter!(x[1:1], x[2:2], mc=:yellow, msc=:yellow, label=\"start\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "julia"
     ],
     "id": ""
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "@time optimize(ro, [0.0, 0.0], Optim.Newton(),Optim.Options(show_trace=false))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time optimize(ro, g!, h!, [0.0, 0.0], Optim.Newton(),Optim.Options(show_trace=false))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "julia"
     ],
     "id": ""
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "@time optimize(ro, g!, h!,  [-1.0, 3.0], BFGS());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# low memory BFGS\n",
    "@time optimize(ro, g!, h!,  [0.0, 0.0], LBFGS());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Direct Methods\n",
    "\n",
    "* No derivative information is used - *derivative free*\n",
    "* If it's very hard / impossible to provide gradient information, this is our only chance.\n",
    "* Direct methods use other criteria than the gradient to inform the next step (and ulimtately convergence)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Cyclic Coordinate Descent -- Taxicab search\n",
    "\n",
    "* We do a line search over each dimension, one after the other\n",
    "* *taxicab* because the path looks like a NYC taxi changing direction at each block.\n",
    "* given $\\mathbf{x}^{(1)}$, we proceed\n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "    \\mathbf{x}^{(2)} &= \\arg \\min_{x_1} f(x_1,x_2^{(1)},\\dots,x_n^{(1)}) \\\\\n",
    "    \\mathbf{x}^{(3)} &= \\arg \\min_{x_2} f(x_1^{(2)},x_2,x_3^{(2)}\\dots,x_n^{(2)}) \\\\    \n",
    "    \\end{aligned}\n",
    "    $$\n",
    "* unfortunately this can easily get stuck because it can only move in 2 directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# start to setup a basis function, i.e. unit vectors to index each direction:\n",
    "basis(i, n) = [k == i ? 1.0 : 0.0 for k in 1 : n]\n",
    "function cyclic_coordinate_descent(f, x, ε) \n",
    "    Δ, n = Inf, length(x)\n",
    "    while abs(Δ) > ε\n",
    "        x′ = copy(x) \n",
    "            for i in 1 : n\n",
    "                d = basis(i, n)\n",
    "                x = line_search(f, x, d) \n",
    "            end\n",
    "        Δ = norm(x - x′) \n",
    "    end\n",
    "    return x \n",
    "end  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### General Pattern Search\n",
    "\n",
    "* We search according to an arbitrary *pattern* $\\mathcal{P}$ of candidate points, anchored at current guess $\\mathbf{x}$.\n",
    "* With step size $\\alpha$ and set $\\mathcal{D}$ of directions\n",
    "    $$ \\mathcal{P} = {\\mathbf{x} + \\alpha \\mathbf{d} \\text{ for } \\mathbf{d}\\in\\mathcal{D} }$$\n",
    "* Convergence is guaranteed under conditions:\n",
    "    * $\\mathcal{D}$ must be a positive spanning set: at least one $\\mathbf{d}\\in\\mathcal{D}$ has a non-zero gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](spanning-set.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "function generalized_pattern_search(f, x, α, D, ε, γ=0.5) \n",
    "    y, n = f(x), length(x)\n",
    "    evals = 0\n",
    "    while α > ε\n",
    "        improved = false\n",
    "        for (i,d) in enumerate(D)\n",
    "            x′ = x + α*d \n",
    "            y′ = f(x′) \n",
    "            evals += 1\n",
    "            if y′ < y\n",
    "                x, y, improved = x′, y′, true\n",
    "                D = pushfirst!(deleteat!(D, i), d) \n",
    "                break\n",
    "            end \n",
    "        end\n",
    "        if !improved \n",
    "            α *= γ\n",
    "        end \n",
    "    end\n",
    "    println(\"$evals evaluations\")\n",
    "    return x \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "D = [[1,0],[0,1],[-1,-0.5]]\n",
    "y=generalized_pattern_search(ro,zeros(2),0.8,D,1e-6 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bracketing for Multidimensional Problems: Nelder-Mead\n",
    "\n",
    "* The Goal here is to find the simplex containing the local minimizer $x^*$\n",
    "* In the case where $f$ is n-D, this simplex has $n+1$ vertices\n",
    "* In the case where $f$ is 2-D, this simplex has $2+1$ vertices, i.e. it's a triangle.\n",
    "* The method proceeds by evaluating the function at all $n+1$ vertices, and by replacing the worst function value with a new guess.\n",
    "* this can be achieved by a sequence of moves:\n",
    "\t* reflect\n",
    "\t* expand\n",
    "\t* contract\n",
    "\t* shrink\n",
    "\tmovements.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](nelder-mead.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "* this is a very popular method. The matlab functions `fmincon` and `fminsearch` implements it.\n",
    "* When it works, it works quite fast.\n",
    "* No derivatives required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "julia"
     ],
     "id": ""
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "@time nm=optimize(ro, [0.0, 0.0], NelderMead());\n",
    "nm.minimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* But.\n",
    "\n",
    "\n",
    "## Bracketing for Multidimensional Problems: Comment on Nelder-Mead\n",
    "\n",
    "> Lagarias et al. (SIOPT, 1999):\n",
    "At present there is no function in any dimension greater than one, for which the original Nelder-Mead algorithm has been proved to converge to a minimizer.\n",
    "\n",
    ">Given all the known inefficiencies and failures of the Nelder-Mead algorithm [...], one might wonder why it is used at all, let alone why it is so extraordinarily popular.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## things to read up on\n",
    "\n",
    "* Divided Rectangles (DIRECT)\n",
    "* simulated annealing and other stochastic gradient methods\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stochastic Optimization Methods\n",
    "\n",
    "* Gradient based methods like steepest descent may be susceptible to getting stuck at local minima.\n",
    "* Randomly shocking the value of the descent direction may be a solution to this.\n",
    "* For example, one could modify our gradient descent from before to become\n",
    "\n",
    "\n",
    " $$\\mathbf{x}^{(k+1)} \\longleftarrow \\mathbf{x}^{(k)} +\\alpha^k\\mathbf{g}^{(k)} + \\mathbf{\\varepsilon}^{(k)}$$\n",
    "\n",
    "* where $\\mathbf{\\varepsilon}^{(k)} \\sim N(0,\\sigma_k^2)$, decreasing with $k$.\n",
    "* This *stochastic gradient descent* is often used when training neural networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Simulated Annealing\n",
    "\n",
    "* We specify a *temperature* that controls the degree of randomness.\n",
    "* At first the temperature is high, letting the search jump around widely. This is to escape local minima.\n",
    "* The temperature is gradually decreased, reducing the step sizes. This is to find the local optimimum in the *best* region.\n",
    "* At every iteration $k$, we accept new point $\\mathbf{x'}$ with\n",
    "\n",
    "$$\n",
    "\\Pr(\\text{accept }\\mathbf{x'}) = \\begin{cases}\n",
    "1 & \\text{if }\\Delta y \\leq0 \\\\\n",
    "\\min(e^{\\Delta y / t},1) & \\text{if }\\Delta y > 0 \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "* here $\\Delta y = f(\\mathbf{x'}) - f(\\mathbf{x})$, and $t$ is the *temperature*.\n",
    "* $\\Pr(\\text{accept }\\mathbf{x'})$ is called the **Metropolis Criterion**, building block of *Accept/Reject* algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# f: function\n",
    "# x: initial point\n",
    "# T: transition distribution\n",
    "# t: temp schedule, k_max: max iterations\n",
    "function simulated_annealing(f, x, T, t, k_max) \n",
    "    y = f(x)\n",
    "    ytrace = zeros(typeof(y),k_max)\n",
    "    x_best, y_best = x, y \n",
    "    for k in 1 : k_max\n",
    "        x′ = x + rand(T)\n",
    "        y′ = f(x′)\n",
    "        Δy = y′ - y\n",
    "        if Δy ≤ 0 || rand() < exp(-Δy/t(k))\n",
    "            x, y = x′, y′ \n",
    "        end\n",
    "        if y′ < y_best\n",
    "            x_best, y_best = x′, y′\n",
    "        end \n",
    "        ytrace[k] = y_best\n",
    "    end\n",
    "    return x_best,ytrace\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "function ackley(x, a=20, b=0.2, c=2π) \n",
    "    d = length(x)\n",
    "    return -a*exp(-b*sqrt(sum(x.^2)/d)) - exp(sum(cos.(c*xi) for xi in x)/d) + a + exp(1)\n",
    "end\n",
    "surface(-30:0.1:30,-30:0.1:30,(x,y)->ackley([x, y]),cbar=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# first with optim!\n",
    "res = Optim.optimize(ackley, [-30.0,-30], [30.0,30.0], [-20.0,-20], SAMIN(), Optim.Options(iterations=10^6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "SAMIN(; nt::Int = 5     # reduce temperature every nt*ns*dim(x_init) evaluations\n",
    "        ns::Int = 5     # adjust bounds every ns*dim(x_init) evaluations\n",
    "        rt::T = 0.9     # geometric temperature reduction factor: when temp changes, new temp is t=rt*t\n",
    "        neps::Int = 5   # number of previous best values the final result is compared to\n",
    "        f_tol::T = 1e-12 # the required tolerance level for function value comparisons\n",
    "        x_tol::T = 1e-6 # the required tolerance level for x\n",
    "        coverage_ok::Bool = false # if false, increase temperature until initial parameter space is covered\n",
    "        verbosity::Int = 0) # scalar: 0, 1, 2 or 3 (default = 0).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "res = Optim.optimize(ackley, [-30.0,-30], [30.0,30.0], [-20.0,-20], SAMIN(nt = 15, rt = 0.4), Optim.Options(iterations=10^6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# now with our hand-rolled version!\n",
    "p = Any[]\n",
    "using Distributions\n",
    "niters = 10000\n",
    "temps = (1,10,50)\n",
    "sigs = (1,5,30)\n",
    "push!(p,[plot(x->i/x,1:niters,title = \"tmp $i\",lw=2,ylims = (0,1),leg = false) for i in temps]...)\n",
    "for sig in sigs, t1 in temps\n",
    "    y = simulated_annealing(ackley,[15,15],MvNormal(2,sig),x->t1/x,niters)[2][:]\n",
    "    push!(p,plot(y,title = \"sig = $sig\",leg=false,lw=1.5,color=\"red\",ylims = (0,20)))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot(p...,layout = (4,3))"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "6958a32a0ff04f11ad1ba18311466a33",
   "lastKernelId": "fcbb5070-fcad-48fd-9f92-9e0a4fccdcbd"
  },
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 1.5.0",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
